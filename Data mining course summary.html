<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ISL Summary - SHUMBUSHO Noel</title>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Merriweather', serif;
      background: linear-gradient(to bottom, #ffffff, #f5f8fa);
      color: #2c3e50;
      padding: 40px;
      line-height: 1.8;
    }
    h1, h2, h3 {
      color: #006699;
    }
    h2 {
      font-size: 24px;
      margin-top: 20px;
    }
    h3 {
      font-size: 20px;
      border-left: 4px solid #007acc;
      padding-left: 10px;
      margin-top: 30px;
    }
    p, li {
      font-size: 16px;
    }
    ul {
      background-color: #eaf4fb;
      padding: 15px;
      border-left: 6px solid #0099cc;
      border-radius: 5px;
    }
    a {
      color: #007acc;
      font-weight: bold;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .footer {
      margin-top: 50px;
      text-align: center;
      font-size: 14px;
      color: #777;
    }
  </style>
</head>
<body>

<h1>Name: SHUMBUSHO Noel</h1>
<p><strong>ID:</strong> 100900</p>

<h2>Summary: An Introduction to Statistical Learning with Applications in Python (Based on ISLP)</h2>

<p>This web page summarizes my personal learning journey through key chapters of the book <em>"An Introduction to Statistical Learning with Applications in Python"</em> by Gareth James and colleagues. The book offers a solid foundation in statistical and machine learning concepts, explained through both theory and applied Python examples. This assignment was prepared for the MSDA9223 course (Data Mining and Information Retrieval) under Dr. Pacifique Nizeyimana.</p>

<p>It includes chapter-wise reflections and lab links from my GitHub repository: 
<a href="https://github.com/Noelshumbusho/data-mining/tree/main">https://github.com/Noelshumbusho/data-mining/tree/main</a></p>

<h3>Table of Contents</h3>
<ul>
  <li>Chapter 2: Foundations of Statistical Learning</li>
  <li>Chapter 3: Linear Regression</li>
  <li>Chapter 4: Classification Methods</li>
  <li>Chapter 5: Resampling Techniques</li>
  <li>Chapter 6: Model Selection and Regularization</li>
  <li>Chapter 8: Tree-Based Approaches</li>
  <li>Chapter 9: Support Vector Machines</li>
  <li>Chapter 10: Deep Learning Models</li>
  <li>Chapter 12: Unsupervised Learning</li>
</ul>

<h3>Chapter 2: Foundations of Statistical Learning</h3>
<p>I learned the basic structure of statistical learning, including the difference between supervised and unsupervised learning. The chapter introduced the bias-variance tradeoff and the role it plays in model performance. The K-Nearest Neighbors (KNN) method helped me appreciate the trade-off between model flexibility and interpretability.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%202.ipynb">Chapter 2 Lab</a></p>

<h3>Chapter 3: Linear Regression</h3>
<p>Here I learned about simple linear regression, which involves one predictor, and multiple linear regression, which handles multiple predictors. I practiced interpreting coefficients, testing hypotheses, and creating interaction terms. These skills laid the foundation for understanding prediction and model evaluation.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%203.ipynb">Chapter 3 Lab</a></p>

<h3>Chapter 4: Classification Methods</h3>
<p>I studied different classification techniques including logistic regression, LDA, QDA, Naive Bayes, and KNN. I learned the assumptions and strengths of each, and how to evaluate classification performance using tools like ROC curves and confusion matrices. This chapter helped me understand probabilistic models and decision-making under uncertainty.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%204.ipynb">Chapter 4 Lab</a></p>

<h3>Chapter 5: Resampling Techniques</h3>
<p>This chapter showed me how to use cross-validation (CV) and bootstrap to estimate model performance. K-fold CV proved more practical and efficient than LOOCV. I used the bootstrap to assess variability and improve model confidence. These methods taught me how to judge the reliability of models.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chatper%205.ipynb">Chapter 5 Lab</a></p>

<h3>Chapter 6: Model Selection and Regularization</h3>
<p>I explored Ridge and LASSO, where Ridge shrinks coefficients but keeps all features, while LASSO can zero out coefficients for feature selection. I also studied PCR, which reduces predictors via principal components, and PLS, which builds components based on predictor-response relationships. These tools helped manage multicollinearity and model complexity.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%206.ipynb">Chapter 6 Lab</a></p>

<h3>Chapter 8: Tree-Based Approaches</h3>
<p>I learned how decision trees work and how ensemble methods like bagging, random forests, and boosting improve accuracy. Bagging reduces variance, random forests add randomness in feature selection, and boosting improves prediction by correcting errors sequentially. These methods taught me the strength of combining weak learners.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%208.ipynb">Chapter 8 Lab</a></p>

<h3>Chapter 9: Support Vector Machines</h3>
<p>SVMs introduced me to margin maximization and kernel methods. When data isn't linearly separable, kernel tricks project it to higher dimensions, enabling linear separation. I learned how to tune the cost parameter and apply RBF and polynomial kernels. This chapter deepened my understanding of complex boundary handling.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%209.ipynb">Chapter 9 Lab</a></p>

<h3>Chapter 10: Deep Learning Models</h3>
<p>I explored neural networks made up of layers of neurons trained with backpropagation and stochastic gradient descent. I learned how hidden layers allow networks to capture complex, non-linear relationships. This chapter revealed the power of deep learning for unstructured data but also highlighted its data and computational demands.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%2010.ipynb">Chapter 10 Lab</a></p>

<h3>Chapter 12: Unsupervised Learning</h3>
<p>PCA helped me reduce dimensionality while preserving variance by creating uncorrelated principal components. I also learned about clustering—K-Means forms clusters by minimizing variance, and Hierarchical Clustering reveals data structure through nested groupings. These tools let me explore structure in unlabeled datasets.</p>
<p>Lab: <a href="https://github.com/Noelshumbusho/data-mining/blob/4986d76ed70f35e8522eec1393f79c31635fe6b1/Chapter%2012.ipynb">Chapter 12 Lab</a></p>

<h3>Summary of Algorithms</h3>
<strong>Regression:</strong>
<ul>
  <li>Linear Regression</li>
  <li>Ridge Regression</li>
  <li>LASSO</li>
  <li>Principal Components Regression (PCR)</li>
  <li>Partial Least Squares (PLS)</li>
</ul>

<strong>Classification:</strong>
<ul>
  <li>Logistic Regression</li>
  <li>LDA</li>
  <li>QDA</li>
  <li>Naive Bayes</li>
  <li>KNN</li>
  <li>Decision Trees</li>
  <li>SVM</li>
</ul>

<strong>Unsupervised Learning:</strong>
<ul>
  <li>Principal Component Analysis (PCA)</li>
  <li>K-Means Clustering</li>
  <li>Hierarchical Clustering</li>
</ul>

<div class="footer">© 2025 SHUMBUSHO Noel – All rights reserved</div>
</body>
</html>
